#!/usr/bin/env python3
"""
BlackRoad Labs â€” Experiment 004
World Artifact Pattern Analysis

Analyzes the corpus of world artifacts generated by Pi fleet nodes.
Identifies themes, entropy of generation, and temporal patterns.

Usage:
  python exp-004-world-analysis.py [--github-token TOKEN] [--output OUTPUT]
"""

import argparse
import json
import re
import sys
from collections import Counter, defaultdict
from datetime import datetime, timezone
from pathlib import Path
from typing import Any


def fetch_worlds(token: str | None = None) -> list[dict]:
    """Fetch world artifacts from GitHub API."""
    import urllib.request
    
    headers = {"Accept": "application/vnd.github+json"}
    if token:
        headers["Authorization"] = f"Bearer {token}"
    
    url = "https://api.github.com/repos/BlackRoad-OS-Inc/blackroad-agents/git/trees/main?recursive=1"
    req = urllib.request.Request(url, headers=headers)
    
    try:
        with urllib.request.urlopen(req, timeout=20) as resp:
            data = json.loads(resp.read())
    except Exception as e:
        print(f"[warn] GitHub fetch failed: {e}", file=sys.stderr)
        return []
    
    pattern = re.compile(r'^([\w-]+)/((\d{8})_(\d{6})_(world|lore|code)_([\w-]+))\.md$')
    worlds = []
    for item in data.get("tree", []):
        m = pattern.match(item.get("path", ""))
        if m:
            dir_, filename, date, time_, type_, slug = m.groups()
            node = "aria64" if dir_ == "worlds" else dir_.replace("-worlds", "")
            ts = datetime.strptime(f"{date}_{time_}", "%Y%m%d_%H%M%S").replace(tzinfo=timezone.utc)
            worlds.append({
                "id": filename,
                "node": node,
                "type": type_,
                "slug": slug,
                "timestamp": ts.isoformat(),
            })
    return sorted(worlds, key=lambda w: w["timestamp"])


def analyze(worlds: list[dict]) -> dict:
    """Run analysis on world artifact corpus."""
    if not worlds:
        return {"error": "no worlds to analyze"}
    
    by_node = Counter(w["node"] for w in worlds)
    by_type = Counter(w["type"] for w in worlds)
    
    # Word frequency across slugs
    all_words = []
    for w in worlds:
        words = re.findall(r'[a-z]+', w["slug"].lower())
        all_words.extend(words)
    top_words = Counter(all_words).most_common(20)
    
    # Generation rate (worlds per hour by day)
    by_day: dict[str, list] = defaultdict(list)
    for w in worlds:
        day = w["timestamp"][:10]
        by_day[day].append(w)
    
    daily_rates = {day: len(items) for day, items in sorted(by_day.items())}
    
    # Node activity windows
    node_first = {}
    node_last = {}
    for w in worlds:
        n = w["node"]
        if n not in node_first:
            node_first[n] = w["timestamp"]
        node_last[n] = w["timestamp"]
    
    # Entropy of type distribution
    import math
    total = len(worlds)
    type_entropy = -sum(
        (c / total) * math.log2(c / total)
        for c in by_type.values() if c > 0
    )
    
    return {
        "total": total,
        "by_node": dict(by_node),
        "by_type": dict(by_type),
        "type_entropy": round(type_entropy, 4),
        "top_words": dict(top_words),
        "daily_rates": daily_rates,
        "node_activity": {
            n: {"first": node_first[n], "last": node_last[n]}
            for n in node_first
        },
    }


def report(analysis: dict) -> None:
    """Print human-readable analysis report."""
    print("\n" + "=" * 60)
    print("  BlackRoad Labs â€” World Artifact Analysis (Exp-004)")
    print("=" * 60)
    print(f"\nğŸ“Š Total Artifacts: {analysis['total']}")
    print(f"ğŸ² Type Entropy: {analysis['type_entropy']:.4f} bits (max={1.585:.3f})")
    
    print("\nğŸŒ By Node:")
    for node, count in sorted(analysis["by_node"].items(), key=lambda x: -x[1]):
        bar = "â–ˆ" * (count * 20 // max(analysis["by_node"].values()))
        print(f"   {node:<12} {bar:<20} {count:>3}")
    
    print("\nğŸ“ By Type:")
    for type_, count in sorted(analysis["by_type"].items(), key=lambda x: -x[1]):
        bar = "â–ˆ" * (count * 20 // max(analysis["by_type"].values()))
        print(f"   {type_:<12} {bar:<20} {count:>3}")
    
    print("\nğŸ”¤ Top Slug Words:")
    words = list(analysis["top_words"].items())[:10]
    for word, count in words:
        print(f"   {word:<16} Ã— {count}")
    
    print("\nğŸ“… Daily Generation Rate:")
    for day, rate in list(analysis["daily_rates"].items())[-7:]:
        bar = "â–ª" * min(rate, 40)
        print(f"   {day}  {bar} {rate}")
    
    print("\nğŸ–¥ï¸  Node Activity Windows:")
    for node, activity in analysis["node_activity"].items():
        print(f"   {node:<12} {activity['first'][:10]} â†’ {activity['last'][:10]}")
    print()


def main():
    parser = argparse.ArgumentParser(description="World Artifact Pattern Analysis")
    parser.add_argument("--token", help="GitHub API token")
    parser.add_argument("--output", help="Save JSON results to file")
    args = parser.parse_args()
    
    print("Fetching world artifacts from GitHub...")
    worlds = fetch_worlds(token=args.token)
    
    if not worlds:
        print("No worlds found. Run with --token for authenticated access.")
        return
    
    analysis = analyze(worlds)
    report(analysis)
    
    if args.output:
        Path(args.output).write_text(json.dumps(analysis, indent=2))
        print(f"Results saved to {args.output}")


if __name__ == "__main__":
    main()
